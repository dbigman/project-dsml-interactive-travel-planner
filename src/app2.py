import chromadb
import openai
from dotenv import load_dotenv
import logging
from icecream import ic
import os
import sys
import streamlit as st

# Print current working directory for debugging
print("Current working directory:", os.getcwd())

# Create a custom logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Clear existing handlers (useful in a notebook if logging was already configured)
if logger.hasHandlers():
    logger.handlers.clear()

# Create handlers: one for file and one for console output
file_handler = logging.FileHandler("landmarks_correction.log")
file_handler.setLevel(logging.INFO)

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)

# Create a formatter and add it to the handlers
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# Add handlers to the logger
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# Load environment variables from .env file
load_dotenv()

# Get OpenAI API key
openai.api_type = "openai"
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    logging.error("OpenAI API key is missing! Check your .env file.")
    raise ValueError("OpenAI API key not found.")

# Since this file is in the src folder, adjust paths to point to the project root
news_client = chromadb.PersistentClient(path="../chromadb")
municipalities_client = chromadb.PersistentClient(path="../chromadb/chromadb_municipalities")
landmarks_client = chromadb.PersistentClient(path="../chromadb/chromadb_landmarks")

# Function to load collections
def load_chromadb_collection(client, collection_name):
    """Load a collection from the ChromaDB client.

    This function attempts to retrieve a specified collection from the
    ChromaDB client. If successful, it returns the collection object. In
    case of an error during the retrieval process, it catches the exception,
    prints an error message, and returns None.

    Args:
        client (ChromaDBClient): The ChromaDB client used to access collections.
        collection_name (str): The name of the collection to be loaded.

    Returns:
        Collection or None: The loaded collection if successful,
        otherwise None.
    """

    try:
        collection = client.get_collection(collection_name)
        print(f"Successfully loaded collection: {collection_name}")
        return collection
    except Exception as e:
        print(f"Error loading collection {collection_name}: {e}")
        return None

# Load collections
news_collection = load_chromadb_collection(news_client, "news_articles")
municipalities_collection = load_chromadb_collection(municipalities_client, "municipalities")
landmarks_collection = load_chromadb_collection(landmarks_client, "landmarks")

# Set OpenAI API Key
openai.api_key = openai_api_key

# Function to perform retrieval from the collections (RAG)
def retrieve_relevant_info(query, collection):
    """Retrieve relevant information from a collection based on a query.

    This function queries the specified collection using the provided query
    string to retrieve a set of relevant documents. It is designed to return
    a limited number of results, which can be adjusted as needed. The
    function assumes that the collection has a method for querying that
    accepts a list of query texts and a parameter for the number of results
    to return.

    Args:
        query (str): The query string used to search for relevant documents.
        collection (Collection): An object representing the collection to query.

    Returns:
        list: A list of documents that are relevant to the provided query.
    """

    # Query the collection with the user's question to get relevant documents
    results = collection.query(query_texts=[query], n_results=3)  # Adjust n_results as needed
    return results

def chat_with_llm(user_input):
    """Interact with a language model to answer user queries based on provided
    context.

    This function retrieves relevant information from various collections
    (municipalities, landmarks, and news) based on the user's input. It
    constructs a prompt that includes the user's query and the combined
    context from the retrieved information. The prompt is then sent to a
    language model to generate a response. If the necessary information is
    not present in the context, the model is instructed to indicate that it
    does not have enough information to answer the question.

    Args:
        user_input (str): The user's query that needs to be answered.

    Returns:
        str: The response generated by the language model based on the provided
            context.
    """

    context = []

    # Retrieve relevant info from each collection
    if municipalities_collection:
        municipalities_info = retrieve_relevant_info(user_input, municipalities_collection)
        context.append(municipalities_info)
    
    if landmarks_collection:
        landmarks_info = retrieve_relevant_info(user_input, landmarks_collection)
        context.append(landmarks_info)
    
    if news_collection:
        news_info = retrieve_relevant_info(user_input, news_collection)
        context.append(news_info)

    # Extract text from results while filtering out None values
    combined_context = "\n".join(
        str(doc) for docs in context if docs and "documents" in docs 
        for doc in docs["documents"][0] if doc is not None
    )

    # Build a new prompt that enforces the constraint:
    prompt = (
        f"{user_input}\n\n"
        f"Context:\n{combined_context}\n\n"
        "Answer the above query **using only the provided context**. "
        "If the necessary information is not present in the context, respond with: "
        "'I do not have enough information to answer this question.'"
    )

    try:
        response = openai.chat.completions.create(
            model="o3-mini-2025-01-31",
            messages=[
                {
                    "role": "system", 
                    "content": (
                        "You are a helpful assistant. "
                        "Answer the user's query using only the information provided in the context. "
                        "Do not incorporate any external knowledge."
                    )
                },
                {"role": "user", "content": prompt}
            ]
        )
        model_reply = response.choices[0].message.content
        return model_reply
    except Exception as e:
        return f"Error: {e}"



st.title("Chat with ChatGPT using RAG system")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": "You are a helpful assistant, always happy to help."}]

# Display chat history
for msg in st.session_state.messages:
    if msg["role"] != "system":  # Skip displaying system message
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

# User input
user_input = st.chat_input("Ask me anything...")
if user_input:
    # Append user message
    st.session_state.messages.append({"role": "user", "content": user_input})

    # Get the response from the LLM
    model_reply = chat_with_llm(user_input)

    # Append AI response
    st.session_state.messages.append({"role": "assistant", "content": model_reply})

    # Display AI response
    with st.chat_message("assistant"):
        st.markdown(model_reply)
