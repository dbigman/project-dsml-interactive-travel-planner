{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbigman/project-dsml-interactive-travel-planner/blob/main/Final_project_IH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHwxqMeySFUv"
      },
      "source": [
        "# The Hithchiker's Guide to Puerto Rico\n",
        "In this project, we are going to use all the Data Science and Machine Learning skills we have acquired during the course of the last few weeks to build an interactive travel planner for the beautiful island of Puerto Rico. By the end of this project, we will present a working application that cooperates with a visitor to help them build a travel itinerary suitable to their personal preferences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HJ3_fVS3Nti4"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import requests\n",
        "\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odkac0ndl0PP"
      },
      "source": [
        "## From HTML to .JSON|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EUAhXnjVatR"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nBNvL9x3Vc2d"
      },
      "outputs": [],
      "source": [
        "# Function to clean unwanted characters and fix words\n",
        "def clean_text(text):\n",
        "    # Remove unwanted characters (e.g., escape sequences like \"\\xc9\")\n",
        "    text = re.sub(r'\\\\[xX][0-9A-Fa-f]{2}', '', text)  # Remove escaped Unicode characters\n",
        "    text = re.sub(r'[\\r\\n\\t]', ' ', text)  # Remove newlines and tabs\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with one\n",
        "    text = text.strip()  # Remove leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "# Function to add missing characters like 'ñ'\n",
        "def add_missing_characters(text):\n",
        "    # Mapping of words that might need 'ñ' or other fixes\n",
        "    replacements = {\n",
        "        \"Aguada\": \"Aguada\",\n",
        "        \"Aasco\": \"Añasco\",\n",
        "        \"Catao\": \"Cataño\",\n",
        "        \"Nio\": \"Niño\",\n",
        "        \"Peuelas\": \"Peñuelas\"\n",
        "        # Add other common words as needed\n",
        "    }\n",
        "\n",
        "    # Replace words based on the mapping\n",
        "    for wrong_word, correct_word in replacements.items():\n",
        "        text = re.sub(rf'\\b{wrong_word}\\b', correct_word, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "# Function to extract coordinates from the HTML content\n",
        "def extract_coordinates(html_content):\n",
        "    # Regex pattern to match lat and lon inside \"wgCoordinates\"\n",
        "    coordinates_pattern = r'\"wgCoordinates\":\\s*\\{\\s*\"lat\":\\s*(-?\\d+\\.\\d+),\\s*\"lon\":\\s*(-?\\d+\\.\\d+)\\s*\\}'\n",
        "    match = re.search(coordinates_pattern, html_content)\n",
        "\n",
        "    if match:\n",
        "        # Clean and extract latitude and longitude as float values\n",
        "        lat = float(match.group(1).strip().replace(\"\\\\n\", \"\"))  # Remove any unwanted newline characters\n",
        "        lon = float(match.group(2).strip().replace(\"\\\\n\", \"\"))  # Remove any unwanted newline characters\n",
        "        return lat, lon\n",
        "\n",
        "    return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OCa74XjRnhg"
      },
      "source": [
        "### Municipalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZWMpCAPlyxi",
        "outputId": "7a427232-e3d3-4d4f-9c85-7f3f88841991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Municipality data with coordinates saved as data\\municipalities\\municipalities.json\n"
          ]
        }
      ],
      "source": [
        "# Path to the municipalities folder in Google Drive\n",
        "# municipalities_folder = \"/content/drive/MyDrive/IronHack_final_project/municipalities\"\n",
        "municipalities_folder = 'data\\municipalities'\n",
        "\n",
        "# List to store structured data\n",
        "municipalities_data = []\n",
        "\n",
        "# Loop through each .txt file in the folder\n",
        "for filename in os.listdir(municipalities_folder):\n",
        "    if filename.endswith(\".txt\"):  # Ensure we only process .txt files\n",
        "        file_path = os.path.join(municipalities_folder, filename)\n",
        "\n",
        "        # Read the HTML content\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            html_content = file.read()\n",
        "\n",
        "        # Parse HTML using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Extract title (or use filename if no title found), and clean it\n",
        "        title = clean_text(soup.title.string) if soup.title else clean_text(filename.replace(\".txt\", \"\"))\n",
        "        # Remove \" - Wikipedia\" from the title\n",
        "        title = title.replace(\" - Wikipedia\", \"\")\n",
        "        title = add_missing_characters(title)  # Add missing characters to the title\n",
        "\n",
        "        # Extract first 3 paragraphs for description\n",
        "        paragraphs = [clean_text(p.get_text(strip=True)) for p in soup.find_all(\"p\")][:3]\n",
        "        paragraphs = [add_missing_characters(p) for p in paragraphs]  # Add missing characters to description\n",
        "\n",
        "        # Extract coordinates (latitude and longitude) from HTML content\n",
        "        latitude, longitude = extract_coordinates(html_content)\n",
        "\n",
        "        # Structure the data\n",
        "        municipality = {\n",
        "            \"name\": title,\n",
        "            \"category\": \"Municipality\",\n",
        "            \"description\": paragraphs,\n",
        "            \"coordinates\": {\n",
        "                \"latitude\": latitude,\n",
        "                \"longitude\": longitude\n",
        "            },\n",
        "            \"source_file\": filename\n",
        "        }\n",
        "\n",
        "        # Append to the list\n",
        "        municipalities_data.append(municipality)\n",
        "\n",
        "# Save structured data as JSON\n",
        "output_json = os.path.join(municipalities_folder, \"municipalities.json\")\n",
        "               \n",
        "with open(output_json, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(municipalities_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Municipality data with coordinates saved as {output_json}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 1/78: Adjuntas, Puerto Rico\n",
            "Processing 2/78: Aguada, Puerto Rico\n",
            "Processing 3/78: Aguadilla, Puerto Rico\n",
            "Processing 4/78: Aguas Buenas, Puerto Rico\n",
            "Processing 5/78: Aibonito, Puerto Rico\n",
            "Processing 6/78: Arecibo, Puerto Rico\n",
            "Processing 7/78: Arroyo, Puerto Rico\n",
            "Processing 8/78: Añasco, Puerto Rico\n",
            "Processing 9/78: Barceloneta, Puerto Rico\n",
            "Processing 10/78: Barranquitas, Puerto Rico\n",
            "Processing 11/78: Bayamn, Puerto Rico\n",
            "Processing 12/78: Cabo Rojo, Puerto Rico\n",
            "Processing 13/78: Caguas, Puerto Rico\n",
            "Processing 14/78: Camuy, Puerto Rico\n",
            "Processing 15/78: Canvanas, Puerto Rico\n",
            "Processing 16/78: Carolina, Puerto Rico\n",
            "Processing 17/78: Cataño, Puerto Rico\n",
            "Processing 18/78: Cayey, Puerto Rico\n",
            "Processing 19/78: Ceiba, Puerto Rico\n",
            "Processing 20/78: Ciales, Puerto Rico\n",
            "Processing 21/78: Cidra, Puerto Rico\n",
            "Processing 22/78: Coamo, Puerto Rico\n",
            "Processing 23/78: Comero, Puerto Rico\n",
            "Processing 24/78: Corozal, Puerto Rico\n",
            "Processing 25/78: Culebra, Puerto Rico\n",
            "Processing 26/78: Dorado, Puerto Rico\n",
            "Processing 27/78: Fajardo, Puerto Rico\n",
            "Processing 28/78: Florida, Puerto Rico\n",
            "Processing 29/78: Guayama, Puerto Rico\n",
            "Processing 30/78: Guayanilla, Puerto Rico\n",
            "Processing 31/78: Guaynabo, Puerto Rico\n",
            "Processing 32/78: Gurabo, Puerto Rico\n",
            "Processing 33/78: Gunica, Puerto Rico\n",
            "Processing 34/78: Hatillo, Puerto Rico\n",
            "Processing 35/78: Hormigueros, Puerto Rico\n",
            "Processing 36/78: Humacao, Puerto Rico\n",
            "Processing 37/78: Isabela, Puerto Rico\n",
            "Processing 38/78: Jayuya, Puerto Rico\n",
            "Processing 39/78: Juana Daz, Puerto Rico\n",
            "Processing 40/78: Juncos, Puerto Rico\n",
            "Processing 41/78: Lajas, Puerto Rico\n",
            "Processing 42/78: Lares, Puerto Rico\n",
            "Processing 43/78: Las Maras, Puerto Rico\n",
            "Processing 44/78: Las Piedras, Puerto Rico\n",
            "Processing 45/78: Loza, Puerto Rico\n",
            "Processing 46/78: Luquillo, Puerto Rico\n",
            "Processing 47/78: Manat, Puerto Rico\n",
            "Processing 48/78: Maricao, Puerto Rico\n",
            "Processing 49/78: Maunabo, Puerto Rico\n",
            "Processing 50/78: Mayagez, Puerto Rico\n",
            "Processing 51/78: Moca, Puerto Rico\n",
            "Processing 52/78: Morovis, Puerto Rico\n",
            "Processing 53/78: Naguabo, Puerto Rico\n",
            "Processing 54/78: Naranjito, Puerto Rico\n",
            "Processing 55/78: Orocovis, Puerto Rico\n",
            "Processing 56/78: Patillas, Puerto Rico\n",
            "Processing 57/78: Peñuelas, Puerto Rico\n",
            "Processing 58/78: Ponce, Puerto Rico\n",
            "Processing 59/78: Quebradillas, Puerto Rico\n",
            "Processing 60/78: Rincn, Puerto Rico\n",
            "Processing 61/78: Ro Grande, Puerto Rico\n",
            "Processing 62/78: Sabana Grande, Puerto Rico\n",
            "Processing 63/78: Salinas, Puerto Rico\n",
            "Processing 64/78: San Germn, Puerto Rico\n",
            "Processing 65/78: San Juan, Puerto Rico\n",
            "Processing 66/78: San Lorenzo, Puerto Rico\n",
            "Processing 67/78: San Sebastin, Puerto Rico\n",
            "Processing 68/78: Santa Isabel, Puerto Rico\n",
            "Processing 69/78: Toa Alta, Puerto Rico\n",
            "Processing 70/78: Toa Baja, Puerto Rico\n",
            "Processing 71/78: Trujillo Alto, Puerto Rico\n",
            "Processing 72/78: Utuado, Puerto Rico\n",
            "Processing 73/78: Vega Alta, Puerto Rico\n",
            "Processing 74/78: Vega Baja, Puerto Rico\n",
            "Processing 75/78: Vieques, Puerto Rico\n",
            "Processing 76/78: Villalba, Puerto Rico\n",
            "Processing 77/78: Yabucoa, Puerto Rico\n",
            "Processing 78/78: Yauco, Puerto Rico\n",
            "Correction process completed! The corrected data is saved in 'municipalities_corrected.json'.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from tabulate import tabulate\n",
        "from json import JSONDecodeError\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=\"municipality_correction.log\",\n",
        "    level=logging.DEBUG,  # Change to logging.INFO, logging.DEBUG, logging.ERROR as needed\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "# Load the JSON file\n",
        "with open(\"data\\municipalities\\municipalities.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get DeepSeek API key\n",
        "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
        "if not deepseek_api_key:\n",
        "    logging.error(\"DeepSeek API key is missing! Check your .env file.\")\n",
        "    raise ValueError(\"DeepSeek API key not found.\")\n",
        "\n",
        "def correct_text_ds(text):\n",
        "    \"\"\"\n",
        "    Corrects typos using deepseek api\n",
        "    \"\"\"\n",
        "    logging.info(\"Calling DeepSeek API...\")\n",
        "    try:\n",
        "        client = openai.OpenAI(\n",
        "            api_key=deepseek_api_key,\n",
        "            base_url=\"https://api.deepseek.com/v1\"  # DeepSeek endpoint\n",
        "        )\n",
        "        prompt = f\"Correct any typos in the following text while keeping the meaning intact. Do not include ANYTHING in addition to the corrected text:\\n{text}\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"deepseek-chat\",  \n",
        "            # model=\"deepseek-reasoner\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a proofreading assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=2000,\n",
        "            top_p=0.8\n",
        "        )\n",
        "        \n",
        "        result = response.choices[0].message.content\n",
        "        logging.info(\"DeepSeek API call successful.\")\n",
        "        logging.info(result)\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected error calling DeepSeek API: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Process each municipality\n",
        "total_municipalities = len(data)\n",
        "for i, municipality in enumerate(data):\n",
        "    print(f\"Processing {i+1}/{total_municipalities}: {municipality['name']}\")\n",
        "    corrected_description = correct_text_ds(\"\".join(municipality[\"description\"]))\n",
        "    municipality[\"description\"] = [corrected_description]\n",
        "\n",
        "# Save the corrected data back to a new JSON file\n",
        "with open(\"municipalities_corrected.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(data, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"Correction process completed! The corrected data is saved in 'municipalities_corrected.json'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFR62hGpRsWe"
      },
      "source": [
        "### Landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQhsrTVqRwDa",
        "outputId": "1bea6fdc-a075-48b8-903a-69084faa683d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Landmark data saved as data\\landmarks\\landmarks.json\n"
          ]
        }
      ],
      "source": [
        "# Path to the landmarks folder in Google Drive\n",
        "landmarks_folder = 'data\\landmarks'\n",
        "\n",
        "# List to store structured data\n",
        "landmarks_data = []\n",
        "\n",
        "# Loop through each .txt file in the folder\n",
        "for filename in os.listdir(landmarks_folder):\n",
        "    if filename.endswith(\".txt\"):  # Ensure we only process .txt files\n",
        "        file_path = os.path.join(landmarks_folder, filename)\n",
        "\n",
        "        # Read the HTML content\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            html_content = file.read()\n",
        "\n",
        "        # Parse HTML using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Extract title (or use filename if no title found)\n",
        "        title = soup.title.string if soup.title else filename.replace(\".txt\", \"\")\n",
        "        title = clean_text(title)  # Clean the title text\n",
        "\n",
        "        # Extract first 3 paragraphs for description\n",
        "        paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")][:3]\n",
        "        paragraphs = [clean_text(paragraph) for paragraph in paragraphs]  # Clean description paragraphs\n",
        "\n",
        "        # Extract coordinates (if available)\n",
        "        lat, lon = extract_coordinates(html_content)\n",
        "        coordinates = {\"latitude\": lat, \"longitude\": lon} if lat and lon else None\n",
        "\n",
        "        # Find the municipality (look for it in the text)\n",
        "        municipality = None\n",
        "        if \"municipality\" in html_content.lower():\n",
        "            municipality_match = re.search(r\"Municipality of\\s+([A-Za-z\\s]+)\", html_content)\n",
        "            if municipality_match:\n",
        "                municipality = clean_text(municipality_match.group(1))  # Clean municipality text\n",
        "\n",
        "        # Structure the data\n",
        "        landmark = {\n",
        "            \"name\": title,\n",
        "            \"category\": \"Landmark\",\n",
        "            \"description\": paragraphs,\n",
        "            \"coordinates\": coordinates,\n",
        "            \"municipality\": municipality,\n",
        "            \"source_file\": filename\n",
        "        }\n",
        "\n",
        "        # Append to the list\n",
        "        landmarks_data.append(landmark)\n",
        "\n",
        "\n",
        "# Save structured data as JSON\n",
        "output_json = os.path.join(landmarks_folder, \"landmarks.json\")\n",
        "with open(output_json, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(landmarks_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Landmark data saved as {output_json}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgI4UouXYvtx"
      },
      "source": [
        "## News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D0DyRRnYyXu",
        "outputId": "286613d0-5248-4638-8b86-10fe4ca49e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total files: 1668\n",
            "Sample files: ['19501216_1.txt', '19430227_1.txt', '19520517_1.txt', '19450331_1.txt', '19340224_1.txt']\n",
            "Average file size: 19467.90 bytes\n",
            "Smallest files: [('19410104_1.txt', 9), ('19380101_1.txt', 9), ('19280901_1.txt', 6673), ('19351221_1.txt', 7570), ('19360125_1.txt', 7782)]\n",
            "Largest files: [('19470802_1.txt', 37101), ('19471004_1.txt', 36966), ('19461221_1.txt', 36070), ('19470524_1.txt', 35724), ('19471206_1.txt', 35615)]\n"
          ]
        }
      ],
      "source": [
        "# Navigate to the correct folder\n",
        "data_dir = \"/content/drive/MyDrive/IronHack_final_project/elmundo_chunked_es_page1_40years\"\n",
        "files = os.listdir(data_dir)\n",
        "\n",
        "print(f\"Total files: {len(files)}\")\n",
        "print(\"Sample files:\", files[:5])  # Preview first 5 files\n",
        "file_sizes = {file: os.path.getsize(os.path.join(data_dir, file)) for file in files}\n",
        "print(f\"Average file size: {sum(file_sizes.values()) / len(file_sizes):.2f} bytes\")\n",
        "print(\"Smallest files:\", sorted(file_sizes.items(), key=lambda x: x[1])[:5])\n",
        "print(\"Largest files:\", sorted(file_sizes.items(), key=lambda x: x[1], reverse=True)[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvHopbTQSlQs"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLl3oTGGSkrG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
