{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbigman/project-dsml-interactive-travel-planner/blob/main/Final_project_IH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Hithchiker's Guide to Puerto Rico\n",
        "In this project, we are going to use all the Data Science and Machine Learning skills we have acquired during the course of the last few weeks to build an interactive travel planner for the beautiful island of Puerto Rico. By the end of this project, we will present a working application that cooperates with a visitor to help them build a travel itinerary suitable to their personal preferences."
      ],
      "metadata": {
        "id": "iHwxqMeySFUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_bk8fZ5g-5_",
        "outputId": "b2d07475-c8c6-4f16-d069-d0d8a537fa9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ3_fVS3Nti4"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# to .json\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From HTML to .JSON|"
      ],
      "metadata": {
        "id": "Odkac0ndl0PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "1EUAhXnjVatR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean unwanted characters and fix words\n",
        "def clean_text(text):\n",
        "    # Remove unwanted characters (e.g., escape sequences like \"\\xc9\")\n",
        "    text = re.sub(r'\\\\[xX][0-9A-Fa-f]{2}', '', text)  # Remove escaped Unicode characters\n",
        "    text = re.sub(r'[\\r\\n\\t]', ' ', text)  # Remove newlines and tabs\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with one\n",
        "    text = text.strip()  # Remove leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "# Function to add missing characters like 'ñ'\n",
        "def add_missing_characters(text):\n",
        "    # Mapping of words that might need 'ñ' or other fixes\n",
        "    replacements = {\n",
        "        \"Aguada\": \"Aguada\",\n",
        "        \"Aasco\": \"Añasco\",\n",
        "        \"Catao\": \"Cataño\",\n",
        "        \"Nio\": \"Niño\",\n",
        "        \"Peuelas\": \"Peñuelas\"\n",
        "        # Add other common words as needed\n",
        "    }\n",
        "\n",
        "    # Replace words based on the mapping\n",
        "    for wrong_word, correct_word in replacements.items():\n",
        "        text = re.sub(rf'\\b{wrong_word}\\b', correct_word, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "# Function to extract coordinates from the HTML content\n",
        "def extract_coordinates(html_content):\n",
        "    # Regex pattern to match lat and lon inside \"wgCoordinates\"\n",
        "    coordinates_pattern = r'\"wgCoordinates\":\\s*\\{\\s*\"lat\":\\s*(-?\\d+\\.\\d+),\\s*\"lon\":\\s*(-?\\d+\\.\\d+)\\s*\\}'\n",
        "    match = re.search(coordinates_pattern, html_content)\n",
        "\n",
        "    if match:\n",
        "        # Clean and extract latitude and longitude as float values\n",
        "        lat = float(match.group(1).strip().replace(\"\\\\n\", \"\"))  # Remove any unwanted newline characters\n",
        "        lon = float(match.group(2).strip().replace(\"\\\\n\", \"\"))  # Remove any unwanted newline characters\n",
        "        return lat, lon\n",
        "\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "nBNvL9x3Vc2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Municipalities"
      ],
      "metadata": {
        "id": "5OCa74XjRnhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the municipalities folder in Google Drive\n",
        "municipalities_folder = \"/content/drive/MyDrive/IronHack_final_project/municipalities\"\n",
        "\n",
        "# List to store structured data\n",
        "municipalities_data = []\n",
        "\n",
        "# Loop through each .txt file in the folder\n",
        "for filename in os.listdir(municipalities_folder):\n",
        "    if filename.endswith(\".txt\"):  # Ensure we only process .txt files\n",
        "        file_path = os.path.join(municipalities_folder, filename)\n",
        "\n",
        "        # Read the HTML content\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            html_content = file.read()\n",
        "\n",
        "        # Parse HTML using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Extract title (or use filename if no title found), and clean it\n",
        "        title = clean_text(soup.title.string) if soup.title else clean_text(filename.replace(\".txt\", \"\"))\n",
        "        # Remove \" - Wikipedia\" from the title\n",
        "        title = title.replace(\" - Wikipedia\", \"\")\n",
        "        title = add_missing_characters(title)  # Add missing characters to the title\n",
        "\n",
        "        # Extract first 3 paragraphs for description\n",
        "        paragraphs = [clean_text(p.get_text(strip=True)) for p in soup.find_all(\"p\")][:3]\n",
        "        paragraphs = [add_missing_characters(p) for p in paragraphs]  # Add missing characters to description\n",
        "\n",
        "        # Extract coordinates (latitude and longitude) from HTML content\n",
        "        latitude, longitude = extract_coordinates(html_content)\n",
        "\n",
        "        # Structure the data\n",
        "        municipality = {\n",
        "            \"name\": title,\n",
        "            \"category\": \"Municipality\",\n",
        "            \"description\": paragraphs,\n",
        "            \"coordinates\": {\n",
        "                \"latitude\": latitude,\n",
        "                \"longitude\": longitude\n",
        "            },\n",
        "            \"source_file\": filename\n",
        "        }\n",
        "\n",
        "        # Append to the list\n",
        "        municipalities_data.append(municipality)\n",
        "\n",
        "# Save structured data as JSON\n",
        "output_json = \"/content/drive/MyDrive/IronHack_final_project/municipalities.json\"\n",
        "with open(output_json, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(municipalities_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Municipality data with coordinates saved as {output_json}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZWMpCAPlyxi",
        "outputId": "7a427232-e3d3-4d4f-9c85-7f3f88841991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Municipality data with coordinates saved as /content/drive/MyDrive/IronHack_final_project/municipalities.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Landmarks"
      ],
      "metadata": {
        "id": "RFR62hGpRsWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the landmarks folder in Google Drive\n",
        "landmarks_folder = \"/content/drive/MyDrive/IronHack_final_project/landmarks\"\n",
        "\n",
        "# List to store structured data\n",
        "landmarks_data = []\n",
        "\n",
        "# Loop through each .txt file in the folder\n",
        "for filename in os.listdir(landmarks_folder):\n",
        "    if filename.endswith(\".txt\"):  # Ensure we only process .txt files\n",
        "        file_path = os.path.join(landmarks_folder, filename)\n",
        "\n",
        "        # Read the HTML content\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            html_content = file.read()\n",
        "\n",
        "        # Parse HTML using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Extract title (or use filename if no title found)\n",
        "        title = soup.title.string if soup.title else filename.replace(\".txt\", \"\")\n",
        "        title = clean_text(title)  # Clean the title text\n",
        "\n",
        "        # Extract first 3 paragraphs for description\n",
        "        paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")][:3]\n",
        "        paragraphs = [clean_text(paragraph) for paragraph in paragraphs]  # Clean description paragraphs\n",
        "\n",
        "        # Extract coordinates (if available)\n",
        "        lat, lon = extract_coordinates(html_content)\n",
        "        coordinates = {\"latitude\": lat, \"longitude\": lon} if lat and lon else None\n",
        "\n",
        "        # Find the municipality (look for it in the text)\n",
        "        municipality = None\n",
        "        if \"municipality\" in html_content.lower():\n",
        "            municipality_match = re.search(r\"Municipality of\\s+([A-Za-z\\s]+)\", html_content)\n",
        "            if municipality_match:\n",
        "                municipality = clean_text(municipality_match.group(1))  # Clean municipality text\n",
        "\n",
        "        # Structure the data\n",
        "        landmark = {\n",
        "            \"name\": title,\n",
        "            \"category\": \"Landmark\",\n",
        "            \"description\": paragraphs,\n",
        "            \"coordinates\": coordinates,\n",
        "            \"municipality\": municipality,\n",
        "            \"source_file\": filename\n",
        "        }\n",
        "\n",
        "        # Append to the list\n",
        "        landmarks_data.append(landmark)\n",
        "\n",
        "# Save structured data as JSON\n",
        "output_json = \"/content/drive/MyDrive/IronHack_final_project/landmarks.json\"\n",
        "with open(output_json, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(landmarks_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Landmark data saved as {output_json}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQhsrTVqRwDa",
        "outputId": "1bea6fdc-a075-48b8-903a-69084faa683d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Landmark data saved as /content/drive/MyDrive/IronHack_final_project/landmarks.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## News"
      ],
      "metadata": {
        "id": "sgI4UouXYvtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the correct folder\n",
        "data_dir = \"/content/drive/MyDrive/IronHack_final_project/elmundo_chunked_es_page1_40years\"\n",
        "files = os.listdir(data_dir)\n",
        "\n",
        "print(f\"Total files: {len(files)}\")\n",
        "print(\"Sample files:\", files[:5])  # Preview first 5 files\n",
        "file_sizes = {file: os.path.getsize(os.path.join(data_dir, file)) for file in files}\n",
        "print(f\"Average file size: {sum(file_sizes.values()) / len(file_sizes):.2f} bytes\")\n",
        "print(\"Smallest files:\", sorted(file_sizes.items(), key=lambda x: x[1])[:5])\n",
        "print(\"Largest files:\", sorted(file_sizes.items(), key=lambda x: x[1], reverse=True)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D0DyRRnYyXu",
        "outputId": "286613d0-5248-4638-8b86-10fe4ca49e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1668\n",
            "Sample files: ['19501216_1.txt', '19430227_1.txt', '19520517_1.txt', '19450331_1.txt', '19340224_1.txt']\n",
            "Average file size: 19467.90 bytes\n",
            "Smallest files: [('19410104_1.txt', 9), ('19380101_1.txt', 9), ('19280901_1.txt', 6673), ('19351221_1.txt', 7570), ('19360125_1.txt', 7782)]\n",
            "Largest files: [('19470802_1.txt', 37101), ('19471004_1.txt', 36966), ('19461221_1.txt', 36070), ('19470524_1.txt', 35724), ('19471206_1.txt', 35615)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "dvHopbTQSlQs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLl3oTGGSkrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}