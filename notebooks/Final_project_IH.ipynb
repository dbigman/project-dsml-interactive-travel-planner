{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbigman/project-dsml-interactive-travel-planner/blob/main/Final_project_IH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHwxqMeySFUv"
      },
      "source": [
        "# The Hithchiker's Guide to Puerto Rico\n",
        "In this project, we are going to use all the Data Science and Machine Learning skills we have acquired during the course of the last few weeks to build an interactive travel planner for the beautiful island of Puerto Rico. By the end of this project, we will present a working application that cooperates with a visitor to help them build a travel itinerary suitable to their personal preferences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HJ3_fVS3Nti4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dbigman\\OneDrive - SUDOC LLC\\Desktop\\GitHub\\Ironhack_bootcamp\\project-dsml-interactive-travel-planner\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import requests\n",
        "\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odkac0ndl0PP"
      },
      "source": [
        "## From HTML to .JSON|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EUAhXnjVatR"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nBNvL9x3Vc2d"
      },
      "outputs": [],
      "source": [
        "# Function to clean unwanted characters and fix words\n",
        "def clean_text(text):\n",
        "    # Remove unwanted characters (e.g., escape sequences like \"\\xc9\")\n",
        "    text = re.sub(r'\\\\[xX][0-9A-Fa-f]{2}', '', text)  # Remove escaped Unicode characters\n",
        "    text = re.sub(r'[\\r\\n\\t]', ' ', text)  # Remove newlines and tabs\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with one\n",
        "    text = text.strip()  # Remove leading and trailing spaces\n",
        "    return text\n",
        "\n",
        "# Function to add missing characters like 'ñ'\n",
        "def add_missing_characters(text):\n",
        "    # Mapping of words that might need 'ñ' or other fixes\n",
        "    replacements = {\n",
        "        \"Aguada\": \"Aguada\",\n",
        "        \"Aasco\": \"Añasco\",\n",
        "        \"Catao\": \"Cataño\",\n",
        "        \"Nio\": \"Niño\",\n",
        "        \"Peuelas\": \"Peñuelas\"\n",
        "        # Add other common words as needed\n",
        "    }\n",
        "\n",
        "    # Replace words based on the mapping\n",
        "    for wrong_word, correct_word in replacements.items():\n",
        "        text = re.sub(rf'\\b{wrong_word}\\b', correct_word, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "# Function to extract coordinates from the HTML content\n",
        "def extract_coordinates(html_content):\n",
        "    # Regex pattern to match lat and lon inside \"wgCoordinates\"\n",
        "    coordinates_pattern = r'\"wgCoordinates\":\\s*\\{\\s*\"lat\":\\s*(-?\\d+\\.\\d+),\\s*\"lon\":\\s*(-?\\d+\\.\\d+)\\s*\\}'\n",
        "    match = re.search(coordinates_pattern, html_content)\n",
        "\n",
        "    if match:\n",
        "        # Clean and extract latitude and longitude as float values\n",
        "        lat = float(match.group(1).strip().replace(\"\\\\n\", \"\"))  # Remove any unwanted newline characters\n",
        "        lon = float(match.group(2).strip().replace(\"\\\\n\", \"\"))  # Remove any unwanted newline characters\n",
        "        return lat, lon\n",
        "\n",
        "    return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OCa74XjRnhg"
      },
      "source": [
        "### Municipalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZWMpCAPlyxi",
        "outputId": "7a427232-e3d3-4d4f-9c85-7f3f88841991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Municipality data with coordinates saved as data/municipalities.json\n"
          ]
        }
      ],
      "source": [
        "# Path to the municipalities folder in Google Drive\n",
        "# municipalities_folder = \"/content/drive/MyDrive/IronHack_final_project/municipalities\"\n",
        "municipalities_folder = 'data\\municipalities'\n",
        "\n",
        "# List to store structured data\n",
        "municipalities_data = []\n",
        "\n",
        "# Loop through each .txt file in the folder\n",
        "for filename in os.listdir(municipalities_folder):\n",
        "    if filename.endswith(\".txt\"):  # Ensure we only process .txt files\n",
        "        file_path = os.path.join(municipalities_folder, filename)\n",
        "\n",
        "        # Read the HTML content\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            html_content = file.read()\n",
        "\n",
        "        # Parse HTML using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Extract title (or use filename if no title found), and clean it\n",
        "        title = clean_text(soup.title.string) if soup.title else clean_text(filename.replace(\".txt\", \"\"))\n",
        "        # Remove \" - Wikipedia\" from the title\n",
        "        title = title.replace(\" - Wikipedia\", \"\")\n",
        "        title = add_missing_characters(title)  # Add missing characters to the title\n",
        "\n",
        "        # Extract first 3 paragraphs for description\n",
        "        paragraphs = [clean_text(p.get_text(strip=True)) for p in soup.find_all(\"p\")][:3]\n",
        "        paragraphs = [add_missing_characters(p) for p in paragraphs]  # Add missing characters to description\n",
        "\n",
        "        # Extract coordinates (latitude and longitude) from HTML content\n",
        "        latitude, longitude = extract_coordinates(html_content)\n",
        "\n",
        "        # Structure the data\n",
        "        municipality = {\n",
        "            \"name\": title,\n",
        "            \"category\": \"Municipality\",\n",
        "            \"description\": paragraphs,\n",
        "            \"coordinates\": {\n",
        "                \"latitude\": latitude,\n",
        "                \"longitude\": longitude\n",
        "            },\n",
        "            \"source_file\": filename\n",
        "        }\n",
        "\n",
        "        # Append to the list\n",
        "        municipalities_data.append(municipality)\n",
        "\n",
        "# Save structured data as JSON\n",
        "\n",
        "output_json = \"data/municipalities.json\"\n",
        "with open(output_json, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(municipalities_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Municipality data with coordinates saved as {output_json}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "muni_data_extended = []\n",
        "for entry in municipalities_data:\n",
        "    muni_data_extended.append({\n",
        "        \"Municipality\": entry[\"name\"],\n",
        "        \"Category\": entry[\"category\"],\n",
        "        \"Description\": \" \".join(entry[\"description\"]).strip(),\n",
        "        \"Latitude\": entry[\"coordinates\"][\"latitude\"],\n",
        "        \"Longitude\": entry[\"coordinates\"][\"longitude\"],\n",
        "        \"Source File\": entry[\"source_file\"]\n",
        "    })\n",
        "\n",
        "municipalities_df = pd.DataFrame(muni_data_extended)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFR62hGpRsWe"
      },
      "source": [
        "### Landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQhsrTVqRwDa",
        "outputId": "1bea6fdc-a075-48b8-903a-69084faa683d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Landmark data with coordinates saved as data/landmarks.json\n"
          ]
        }
      ],
      "source": [
        "# Path to the landmarks folder in Google Drive\n",
        "landmarks_folder = 'data\\landmarks'\n",
        "\n",
        "# List to store structured data\n",
        "landmarks_data = []\n",
        "\n",
        "# Loop through each .txt file in the folder\n",
        "for filename in os.listdir(landmarks_folder):\n",
        "    if filename.endswith(\".txt\"):  # Ensure we only process .txt files\n",
        "        file_path = os.path.join(landmarks_folder, filename)\n",
        "\n",
        "        # Read the HTML content\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            html_content = file.read()\n",
        "\n",
        "        # Parse HTML using BeautifulSoup\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "        # Extract title (or use filename if no title found), and clean it\n",
        "        title = clean_text(soup.title.string) if soup.title else clean_text(filename.replace(\".txt\", \"\"))\n",
        "        # Remove \" - Wikipedia\" from the title\n",
        "        title = title.replace(\" - Wikipedia\", \"\")\n",
        "        title = add_missing_characters(title)  # Add missing characters to the title\n",
        "\n",
        "        # Extract first 3 paragraphs for description\n",
        "        paragraphs = [clean_text(p.get_text(strip=True)) for p in soup.find_all(\"p\")][:3]\n",
        "        paragraphs = [add_missing_characters(p) for p in paragraphs]  # Add missing characters to description\n",
        "\n",
        "        # Extract coordinates (latitude and longitude) from HTML content\n",
        "        latitude, longitude = extract_coordinates(html_content)\n",
        "\n",
        "        # Structure the data\n",
        "        landmark = {\n",
        "            \"name\": title,\n",
        "            \"category\": \"Landmark\",\n",
        "            \"description\": paragraphs,\n",
        "            \"coordinates\": {\n",
        "                \"latitude\": latitude,\n",
        "                \"longitude\": longitude\n",
        "            },\n",
        "            \"source_file\": filename\n",
        "        }\n",
        "\n",
        "        # Append to the list\n",
        "        landmarks_data.append(landmark)\n",
        "\n",
        "# Save structured data as JSON\n",
        "output_json = \"data/landmarks.json\"\n",
        "with open(output_json, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(landmarks_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"Landmark data with coordinates saved as {output_json}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "import sys\n",
        "from icecream import ic\n",
        "\n",
        "# Create a custom logger\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Clear existing handlers (useful in a notebook if logging was already configured)\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# Create handlers: one for file and one for console output\n",
        "file_handler = logging.FileHandler(\"landmarks_correction.log\")\n",
        "file_handler.setLevel(logging.INFO)\n",
        "\n",
        "console_handler = logging.StreamHandler(sys.stdout)\n",
        "console_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Create a formatter and add it to the handlers\n",
        "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "file_handler.setFormatter(formatter)\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add handlers to the logger\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get OpenAI API key\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai_api_key:\n",
        "    logging.error(\"OpenAI API key is missing! Check your .env file.\")\n",
        "    raise ValueError(\"OpenAI API key not found.\")\n",
        "\n",
        "def correct_text(text, retries=3):\n",
        "    \"\"\"\n",
        "    Corrects typos using OpenAI API with simple retry logic.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        f\"Correct any typos in the following text while keeping the meaning intact. \"\n",
        "        f\"Do not include ANYTHING in addition to the corrected text:\\n{text}\"\n",
        "    )\n",
        "    \n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            logging.info(\"Calling OpenAI API (attempt %s)...\", attempt)\n",
        "            client = openai.OpenAI(\n",
        "                api_key=openai_api_key,\n",
        "                base_url=\"https://api.openai.com/v1\"\n",
        "            )\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini-2024-07-18\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a proofreading assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=1,\n",
        "                max_tokens=500,\n",
        "                top_p=1\n",
        "            )\n",
        "            \n",
        "            result = response.choices[0].message.content.strip()\n",
        "            logging.info(\"OpenAI API call successful on attempt %s.\", attempt)\n",
        "            logging.debug(\"Result: %s\", result)\n",
        "            return result\n",
        "        \n",
        "        except Exception as e:\n",
        "            logging.error(\"Error on attempt %s: %s\", attempt, e)\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "    \n",
        "    logging.error(\"All attempts failed for text starting with: %s\", text[:50])\n",
        "    ic(text)\n",
        "    return text  \n",
        "\n",
        "def process_description(text):\n",
        "    \"\"\"\n",
        "    Wrapper function to process each description. Inserts a short delay\n",
        "    to help avoid rate limiting.\n",
        "    \"\"\"\n",
        "    corrected = correct_text(text)\n",
        "    ic(corrected)\n",
        "    time.sleep(1)  # Short delay to avoid rate limiting\n",
        "    return corrected\n",
        "\n",
        "# Load the landmarks dataset\n",
        "# landmarks_df = pd.read_csv(\"landmarks.csv\", encoding=\"utf-8\")\n",
        "\n",
        "# Load the JSON file\n",
        "landmarks_json_path = \"data/landmarks.json\"\n",
        "\n",
        "with open(landmarks_json_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    landmarks_data = json.load(file)\n",
        "\n",
        "# Convert JSON data into a Pandas DataFrame\n",
        "landmarks_df = pd.DataFrame(landmarks_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply the correction function to the 'description' column and store results in a new column\n",
        "landmarks_df['corrected_description'] = landmarks_df['description'].apply(process_description)\n",
        "\n",
        "# Optionally, save the updated DataFrame to a new CSV file\n",
        "landmarks_df.to_csv(\"landmarks_corrected.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Correction process completed! The corrected data is saved in 'landmarks_corrected.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the corrected DataFrame as a JSON file\n",
        "landmarks_df.to_json(\"landmarks_corrected.json\", orient=\"records\", force_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgI4UouXYvtx"
      },
      "source": [
        "## News"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D0DyRRnYyXu",
        "outputId": "286613d0-5248-4638-8b86-10fe4ca49e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total files: 1668\n",
            "Sample files: ['19501216_1.txt', '19430227_1.txt', '19520517_1.txt', '19450331_1.txt', '19340224_1.txt']\n",
            "Average file size: 19467.90 bytes\n",
            "Smallest files: [('19410104_1.txt', 9), ('19380101_1.txt', 9), ('19280901_1.txt', 6673), ('19351221_1.txt', 7570), ('19360125_1.txt', 7782)]\n",
            "Largest files: [('19470802_1.txt', 37101), ('19471004_1.txt', 36966), ('19461221_1.txt', 36070), ('19470524_1.txt', 35724), ('19471206_1.txt', 35615)]\n"
          ]
        }
      ],
      "source": [
        "# Navigate to the correct folder\n",
        "data_dir = \"/content/drive/MyDrive/IronHack_final_project/elmundo_chunked_es_page1_40years\"\n",
        "files = os.listdir(data_dir)\n",
        "\n",
        "print(f\"Total files: {len(files)}\")\n",
        "print(\"Sample files:\", files[:5])  # Preview first 5 files\n",
        "file_sizes = {file: os.path.getsize(os.path.join(data_dir, file)) for file in files}\n",
        "print(f\"Average file size: {sum(file_sizes.values()) / len(file_sizes):.2f} bytes\")\n",
        "print(\"Smallest files:\", sorted(file_sizes.items(), key=lambda x: x[1])[:5])\n",
        "print(\"Largest files:\", sorted(file_sizes.items(), key=lambda x: x[1], reverse=True)[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvHopbTQSlQs"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLl3oTGGSkrG"
      },
      "outputs": [],
      "source": [
        "def read_text_file(file_path, num_lines=15):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return \"\\n\".join([next(f) for _ in range(num_lines)])\n",
        "\n",
        "sample_file = os.path.join(data_dir, files[0])\n",
        "print(f\"Contents of {files[0]}:\\n\", read_text_file(sample_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "file_lengths = []\n",
        "\n",
        "for file in files:\n",
        "    with open(os.path.join(data_dir, file), \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "        file_lengths.append(len(text.split()))  # Count words\n",
        "\n",
        "print(f\"Average words per file: {np.mean(file_lengths):.2f}\")\n",
        "print(f\"Min words: {np.min(file_lengths)}, Max words: {np.max(file_lengths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib as plt\n",
        "\n",
        "plt.hist(file_lengths, bins=50, edgecolor=\"black\")\n",
        "plt.xlabel(\"Word Count per File\")\n",
        "plt.ylabel(\"Number of Files\")\n",
        "plt.title(\"Distribution of Document Lengths\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
