{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonschema\n",
    "from jsonschema import validate\n",
    "\n",
    "# Define the directory containing .txt files (using a raw string or forward slashes to avoid escape issues)\n",
    "directory = r\"data/municipalities\"  # Adjust path as needed\n",
    "\n",
    "# Define the path to the JSON schema\n",
    "schema_path = r\"municipality_schema.json\"  # Adjust path as needed\n",
    "\n",
    "# Load the JSON schema\n",
    "try:\n",
    "    with open(schema_path, \"r\", encoding=\"utf-8\") as schema_file:\n",
    "        schema = json.load(schema_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Schema file not found: {schema_path}\")\n",
    "    raise\n",
    "\n",
    "def convert_txt_to_json(file_path):\n",
    "    \"\"\"\n",
    "    Reads a text file and attempts to convert its content to a JSON object.\n",
    "    Returns the JSON data if successful; otherwise, returns None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read().strip()  # Remove extra spaces/newlines\n",
    "            if not content:\n",
    "                print(f\"❌ {file_path} is empty. Skipping...\")\n",
    "                return None\n",
    "            try:\n",
    "                return json.loads(content)  # Try parsing as JSON\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"❌ {file_path} does not contain valid JSON. Skipping...\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def validate_municipality(file_path):\n",
    "    \"\"\"\n",
    "    Converts a .txt file to JSON and validates it against the schema.\n",
    "    Returns the JSON data if valid; otherwise, returns None.\n",
    "    \"\"\"\n",
    "    data = convert_txt_to_json(file_path)\n",
    "    if data is None:\n",
    "        print('txt Data not valid. ')\n",
    "        return None  # Skip invalid JSON files\n",
    "\n",
    "    try:\n",
    "        validate(instance=data, schema=schema)\n",
    "        print(f\"✅ {file_path} is valid\")\n",
    "        return data  # Return the parsed JSON if valid\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        print(f\"❌ {file_path} does not match the schema: {e.message}\")\n",
    "        return None\n",
    "\n",
    "# Process all .txt files in the directory\n",
    "municipality_data = []\n",
    "\n",
    "if os.path.isdir(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            data = validate_municipality(file_path)\n",
    "            if data:\n",
    "                municipality_data.append(data)\n",
    "else:\n",
    "    print(f\"❌ Directory not found: {directory}\")\n",
    "\n",
    "# Save the combined valid JSON data into a single file if any valid data exists\n",
    "if municipality_data:\n",
    "    output_file = \"validated_municipalities.json\"\n",
    "    try:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            json.dump(municipality_data, outfile, indent=2)\n",
    "        print(f\"✅ All valid municipalities saved to '{output_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error writing output file: {e}\")\n",
    "else:\n",
    "    print(\"No valid municipality data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ data\\municipalities\\Adjuntas.txt does not contain valid JSON. Skipping...\n"
     ]
    }
   ],
   "source": [
    "muni_txt = r'data\\municipalities\\Adjuntas.txt'\n",
    "\n",
    "\n",
    "\n",
    "json_muni_file = convert_txt_to_json(muni_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data extracted and saved to adjuntas.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_24360\\3017286154.py:21: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  population_tag = soup.find(text=\"• Total\")\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_24360\\3017286154.py:34: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  mayor_tag = soup.find(text=\"• Mayor\")\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_24360\\3017286154.py:40: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  nicknames_tag = soup.find(text=\"Nicknames:\")\n",
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_24360\\3017286154.py:46: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  barrios_section = soup.find(\"h3\", text=\"Barrios\")\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Load  file\n",
    "html_file_path = r'data\\municipalities\\Adjuntas.txt'\n",
    "\n",
    "with open(html_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Extracting key information\n",
    "data = {}\n",
    "\n",
    "# Extracting title (municipality name)\n",
    "title = soup.find(\"title\")\n",
    "data[\"name\"] = title.text.replace(\" - Wikipedia\", \"\") if title else \"Unknown\"\n",
    "\n",
    "# Extracting population (from the infobox)\n",
    "population_tag = soup.find(text=\"• Total\")\n",
    "if population_tag:\n",
    "    population_value = population_tag.find_next(\"td\").text.strip()\n",
    "    data[\"population\"] = population_value\n",
    "\n",
    "# Extracting geographical coordinates\n",
    "coordinates_tag = soup.find(\"span\", class_=\"geo-dec\")\n",
    "if coordinates_tag:\n",
    "    coords = coordinates_tag.text.strip().split(\" \")\n",
    "    data[\"latitude\"] = coords[0]\n",
    "    data[\"longitude\"] = coords[1]\n",
    "\n",
    "# Extracting mayor (from infobox)\n",
    "mayor_tag = soup.find(text=\"• Mayor\")\n",
    "if mayor_tag:\n",
    "    mayor_value = mayor_tag.find_next(\"td\").text.strip()\n",
    "    data[\"mayor\"] = mayor_value\n",
    "\n",
    "# Extracting nicknames\n",
    "nicknames_tag = soup.find(text=\"Nicknames:\")\n",
    "if nicknames_tag:\n",
    "    nicknames_value = nicknames_tag.find_next(\"td\").text.strip()\n",
    "    data[\"nicknames\"] = nicknames_value.split(\", \")\n",
    "\n",
    "# Extracting Barrios\n",
    "barrios_section = soup.find(\"h3\", text=\"Barrios\")\n",
    "if barrios_section:\n",
    "    barrios_list = []\n",
    "    for li in barrios_section.find_next(\"ul\").find_all(\"li\"):\n",
    "        barrios_list.append(li.text.strip())\n",
    "    data[\"barrios\"] = barrios_list\n",
    "\n",
    "# Save the extracted data as JSON\n",
    "json_output_path = \"adjuntas.json\"\n",
    "with open(json_output_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data, json_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Data extracted and saved to {json_output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# sections to extract.\n",
    "target_sections = [\n",
    "    \"Etymology and nicknames\",\n",
    "    \"History\",\n",
    "    \"Geography\",\n",
    "    \"Demographics\",\n",
    "    \"Special Communities\",\n",
    "    \"Economy\",\n",
    "    \"Human resources\",\n",
    "    \"Culture\",\n",
    "    \"Transportation\",\n",
    "    \"Government\",\n",
    "    \"Symbols\"\n",
    "]\n",
    "\n",
    "# Load  file\n",
    "html_file_path = r'data\\municipalities\\Adjuntas.txt'\n",
    "\n",
    "with open(html_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# # Open and read the HTML file\n",
    "# with open(\"adjuntas.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     html_content = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Locate the main content container.\n",
    "content_div = soup.find(\"div\", class_=\"mw-parser-output\")\n",
    "\n",
    "# Dictionary to hold section text\n",
    "sections_data = {}\n",
    "current_section = None\n",
    "\n",
    "# Iterate over direct children of the content container\n",
    "for element in content_div.children:\n",
    "    # When we hit an h2, check if its headline is one of the target sections.\n",
    "    if element.name == \"h2\":\n",
    "        # Many h2 tags in Wikipedia have a structure like:\n",
    "        #   <h2><span class=\"mw-headline\" id=\"...\">Section Title</span><span class=\"mw-editsection\">[ edit ]</span></h2>\n",
    "        # We extract the text and remove the \"[ edit ]\" part.\n",
    "        headline = element.get_text(separator=\" \", strip=True).split(\"[\")[0].strip()\n",
    "        # Look for a matching target section (case-insensitive substring match)\n",
    "        match = None\n",
    "        for sec in target_sections:\n",
    "            if sec.lower() in headline.lower():\n",
    "                match = sec\n",
    "                break\n",
    "        current_section = match  # may be None if not a target section\n",
    "        if current_section:\n",
    "            sections_data[current_section] = []  # initialize list of content\n",
    "    # If we are currently in a target section, accumulate text from the element\n",
    "    elif current_section:\n",
    "        # Some elements (like navigation, tables, etc.) might produce unwanted text.\n",
    "        # You can add additional filtering if needed.\n",
    "        text = element.get_text(separator=\" \", strip=True)\n",
    "        if text:\n",
    "            sections_data[current_section].append(text)\n",
    "\n",
    "# Build a list of dictionaries for each section.\n",
    "results = []\n",
    "for section, texts in sections_data.items():\n",
    "    full_text = \"\\n\".join(texts)\n",
    "    results.append({\"Section\": section, \"Content\": full_text})\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 13...\n",
      "Batch 1 processed successfully.\n",
      "Processing batch 2 of 13...\n",
      "Batch 2 processed successfully.\n",
      "Processing batch 3 of 13...\n",
      "Batch 3 processed successfully.\n",
      "Processing batch 4 of 13...\n",
      "Batch 4 processed successfully.\n",
      "Processing batch 5 of 13...\n",
      "Batch 5 processed successfully.\n",
      "Processing batch 6 of 13...\n",
      "Batch 6 processed successfully.\n",
      "Processing batch 7 of 13...\n",
      "Batch 7 processed successfully.\n",
      "Processing batch 8 of 13...\n",
      "Batch 8 processed successfully.\n",
      "Processing batch 9 of 13...\n",
      "Batch 9 processed successfully.\n",
      "Processing batch 10 of 13...\n",
      "Batch 10 processed successfully.\n",
      "Processing batch 11 of 13...\n",
      "Batch 11 processed successfully.\n",
      "Processing batch 12 of 13...\n",
      "Batch 12 processed successfully.\n",
      "Processing batch 13 of 13...\n",
      "Batch 13 processed successfully.\n",
      "+----+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|    |   Batch | Extraction                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|----+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 |       1 | The target sections extracted from the provided HTML (Batch 1) are:                                                                                                                                                                                                                                                                                                                                  |\n",
      "|    |         |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|    |         | **\"Etymology and nicknames\", \"History\", \"Geography\", \"Demographics\", \"Special Communities\", \"Economy\", \"Human resources\"**.                                                                                                                                                                                                                                                                          |\n",
      "|    |         |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|    |         | Sections such as **\"Culture\", \"Transportation\", \"Government\", \"Symbols\"** are not present in the HTML snippet provided and may require analysis of subsequent batches for extraction.                                                                                                                                                                                                                |\n",
      "|  1 |       2 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|  2 |       3 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|  3 |       4 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|  4 |       5 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|  5 |       6 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|  6 |       7 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|  7 |       8 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|  8 |       9 | The provided HTML snippet consists of citation references and does not contain the main article sections requested (Etymology, History, Geography, etc.). To extract the target sections, the HTML containing the actual body content with those headings is required. The current input only includes bibliographic citations supporting the article's content rather than the sections themselves. |\n",
      "|  9 |      10 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "| 10 |      11 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "| 11 |      12 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "| 12 |      13 |                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+----+---------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Combined Extraction:\n",
      "\n",
      "The target sections extracted from the provided HTML (Batch 1) are:\n",
      "\n",
      "**\"Etymology and nicknames\", \"History\", \"Geography\", \"Demographics\", \"Special Communities\", \"Economy\", \"Human resources\"**.\n",
      "\n",
      "Sections such as **\"Culture\", \"Transportation\", \"Government\", \"Symbols\"** are not present in the HTML snippet provided and may require analysis of subsequent batches for extraction.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The provided HTML snippet consists of citation references and does not contain the main article sections requested (Etymology, History, Geography, etc.). To extract the target sections, the HTML containing the actual body content with those headings is required. The current input only includes bibliographic citations supporting the article's content rather than the sections themselves.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tabulate import tabulate\n",
    "from json import JSONDecodeError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"deepseek_extraction.log\",\n",
    "    level=logging.DEBUG,  # Change to logging.INFO or logging.ERROR if needed\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load HTML file\n",
    "html_file_path = r'data\\municipalities\\Adjuntas.txt'\n",
    "try:\n",
    "    with open(html_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "    logging.info(f\"Successfully loaded HTML content from {html_file_path}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading HTML file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Get DeepSeek API key\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "if not deepseek_api_key:\n",
    "    logging.error(\"DeepSeek API key is missing! Check your .env file.\")\n",
    "    raise ValueError(\"DeepSeek API key not found.\")\n",
    "\n",
    "def chatbot_deepseek(prompt):\n",
    "    \"\"\"\n",
    "    Interacts with DeepSeek's API with medium creativity.\n",
    "    \"\"\"\n",
    "    logging.info(\"Calling DeepSeek API...\")\n",
    "    try:\n",
    "        client = openai.OpenAI(\n",
    "            api_key=deepseek_api_key,\n",
    "            base_url=\"https://api.deepseek.com/v1\"  # DeepSeek endpoint\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-reasoner\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an intelligent chatbot.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=2000,\n",
    "            top_p=0.8\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        logging.info(\"DeepSeek API call successful.\")\n",
    "        return result\n",
    "\n",
    "    except JSONDecodeError as e:\n",
    "        logging.error(f\"JSON Decode Error in DeepSeek response: {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error calling DeepSeek API: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Define the base prompt with instructions\n",
    "base_prompt = '''\n",
    "Read this HTML. The body of the article is divided into multiple sections, each corresponding to a part of the topic.\n",
    "Extract these target sections and return as a string:\n",
    "\n",
    "\"Etymology and nicknames\",\n",
    "\"History\",\n",
    "\"Geography\",\n",
    "\"Demographics\",\n",
    "\"Special Communities\",\n",
    "\"Economy\",\n",
    "\"Human resources\",\n",
    "\"Culture\",\n",
    "\"Transportation\",\n",
    "\"Government\",\n",
    "\"Symbols\"\n",
    "'''\n",
    "\n",
    "# Set a batch size (number of characters per batch) and overlap to avoid cutting sections.\n",
    "batch_size = 30000\n",
    "overlap = 1000  # Characters of overlap\n",
    "\n",
    "# Split the HTML into overlapping chunks\n",
    "chunks = []\n",
    "start = 0\n",
    "while start < len(html_content):\n",
    "    end = start + batch_size\n",
    "    chunk = html_content[start:end]\n",
    "\n",
    "    # Add overlap if not at the end\n",
    "    if end < len(html_content):\n",
    "        chunk += html_content[end:end+overlap]\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "    start += batch_size\n",
    "\n",
    "logging.info(f\"HTML split into {len(chunks)} batches.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process each chunk individually\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    prompt_with_chunk = f\"{base_prompt}\\n\\nBatch {idx+1}:\\n{chunk}\"\n",
    "    logging.info(f\"Processing batch {idx+1} of {len(chunks)}...\")\n",
    "    print(f\"Processing batch {idx+1} of {len(chunks)}...\")\n",
    "\n",
    "    try:\n",
    "        response = chatbot_deepseek(prompt_with_chunk)\n",
    "        results.append(response)\n",
    "        logging.info(f\"Batch {idx+1} processed successfully.\")\n",
    "        print(f\"Batch {idx+1} processed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing batch {idx+1}: {e}\")\n",
    "        print(f\"Error processing batch {idx+1}: {e}\")\n",
    "        results.append(\"\")\n",
    "    \n",
    "    # Sleep a bit to avoid rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Combine all extracted sections\n",
    "final_extraction = \"\\n\".join(results)\n",
    "logging.info(\"All batches processed. Extraction complete.\")\n",
    "\n",
    "# Store results in a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Batch\": list(range(1, len(results)+1)),\n",
    "    \"Extraction\": results\n",
    "})\n",
    "\n",
    "# Save to file\n",
    "df.to_csv(\"deepseek_extracted_sections.csv\", index=False, encoding=\"utf-8\")\n",
    "logging.info(\"Extraction results saved to deepseek_extracted_sections.csv.\")\n",
    "\n",
    "# Print results\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n",
    "print(\"\\nCombined Extraction:\\n\")\n",
    "print(final_extraction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
